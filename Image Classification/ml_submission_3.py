# -*- coding: utf-8 -*-
"""ML Submission-3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SI-iU6RQBPudkBb7Sf_reSqfw5ASvaXn

# **IMAGE CLASSIFICATION MODEL DEPLOYMENT**

---

###**PERSONAL IDENTITY**

Nama : Mukhamad Azis Tholib \
Email: mukhamadazistholib278@gmail.com

### **DOWNLOAD DATASET FROM KAGGLE**
"""

# kaggle instalation package

!pip install -q kaggle

# upload kaggle.json
from google.colab import files
files.upload()

# make directory and change permission for kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# download dataset with 'copy api command' from kaggle

!kaggle datasets download -d madisona/translated-animals10

# unzip dataset

!mkdir animals
!unzip -qq translated-animals10.zip -d animals
!ls animals

"""### **IMPORT LIBRARY**"""

# import library

from io import BytesIO
from IPython.display import Image as IMG
from google.colab import files
from keras.preprocessing import image
from shutil import copyfile
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import RMSprop
from urllib.request import urlopen
import seaborn as sns
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import zipfile, os
import warnings
warnings.filterwarnings("ignore")

"""### **LOAD DATASET**"""

# inspect dataset
animals = ['chicken', 'spider', 'sheep', 'butterfly', 'elephant', 'cat']

for animal in animals:
    print(f'{animal} images: ', len(os.listdir(f'/content/animals/animals10/raw-img/{animal}')))

# making folder

try:
  os.mkdir('/content/animals/animals10/training')
  os.mkdir('/content/animals/animals10/testing')
  for animal in animals:
    os.mkdir(f'/content/animals/animals10/training/{animal}')
    os.mkdir(f'/content/animals/animals10/testing/{animal}')
except OSError:
  pass

"""### **DATA SPLIT**"""

# splitting dataset

def split_data(images_path, training_path, testing_path, split_size):
    files = []
    for filename in os.listdir(images_path):
        file = images_path + filename
        if os.path.getsize(file) > 0:
            files.append(filename)
        else:
            print(filename + " others")

    training_length = int(len(files) * split_size)
    testing_length = int(len(files) - training_length)
    shuffled_set = random.sample(files, len(files))
    training_set = shuffled_set[0:training_length]
    testing_set = shuffled_set[training_length:]

    for filename in training_set:
        this_file = images_path + filename
        destination = training_path + filename
        copyfile(this_file, destination)

    for filename in testing_set:
        this_file = images_path + filename
        destination = testing_path + filename
        copyfile(this_file, destination)

# 80% Data Training and 20% Data Validation

split_size = 0.8 

chicken_images_path = "/content/animals/animals10/raw-img/chicken/"
chicken_train_path = "/content/animals/animals10/training/chicken/"
chicken_test_path = "/content/animals/animals10/testing/chicken/"
split_data(chicken_images_path, chicken_train_path, chicken_test_path, split_size)

spider_images_path = "/content/animals/animals10/raw-img/spider/"
spider_train_path = "/content/animals/animals10/training/spider/"
spider_test_path = "/content/animals/animals10/testing/spider/"
split_data(spider_images_path, spider_train_path, spider_test_path, split_size)

sheep_images_path = "/content/animals/animals10/raw-img/sheep/"
sheep_train_path = "/content/animals/animals10/training/sheep/"
sheep_test_path = "/content/animals/animals10/testing/sheep/"
split_data(sheep_images_path, sheep_train_path, sheep_test_path, split_size)

butterfly_images_path = "/content/animals/animals10/raw-img/butterfly/"
butterfly_train_path = "/content/animals/animals10/training/butterfly/"
butterfly_test_path = "/content/animals/animals10/testing/butterfly/"
split_data(butterfly_images_path, butterfly_train_path, butterfly_test_path, split_size)

elephant_images_path = "/content/animals/animals10/raw-img/elephant/"
elephant_train_path = "/content/animals/animals10/training/elephant/"
elephant_test_path = "/content/animals/animals10/testing/elephant/"
split_data(elephant_images_path, elephant_train_path, elephant_test_path, split_size)

cat_images_path = "/content/animals/animals10/raw-img/cat/"
cat_train_path = "/content/animals/animals10/training/cat/"
cat_test_path = "/content/animals/animals10/testing/cat/"
split_data(cat_images_path, cat_train_path, cat_test_path, split_size)

# validation size of dataset

total_train = len(os.listdir(chicken_train_path)) + len(os.listdir(spider_train_path)) + \
              len(os.listdir(sheep_train_path)) + len(os.listdir(butterfly_train_path)) + \
              len(os.listdir(elephant_train_path)) + len(os.listdir(cat_train_path))
total_test  = len(os.listdir(chicken_test_path)) + len(os.listdir(spider_test_path)) + \
              len(os.listdir(sheep_test_path)) + len(os.listdir(butterfly_test_path)) + \
              len(os.listdir(elephant_test_path)) + len(os.listdir(cat_test_path))

print("We have a total training data of " + str(total_train) + " rows and validation data of " + str(total_test))

"""### **DATA MODEL**

**DATA TRAIN**
"""

# image augmentation dataset

TRAINING_DIR = '/content/animals/animals10/training'
train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 30,
    width_shift_range = 0.1,
    height_shift_range = 0.2,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = True,
    fill_mode = 'nearest'
) 

train_generator = train_datagen.flow_from_directory(
    TRAINING_DIR, 
    batch_size = 64,
    class_mode = 'categorical',
    target_size = (150,150)
)

VALIDATION_DIR = "/content/animals/animals10/testing"
validation_datagen = ImageDataGenerator(rescale = 1./255)
validation_generator = validation_datagen.flow_from_directory(
    VALIDATION_DIR,
    batch_size = 64,
    class_mode = 'categorical',
    target_size = (150, 150)
)

"""**CALLBACKS**"""

# callbacks implementation

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.2,
    patience=5, 
    min_lr=1.5e-5
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=12,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True
)

"""**PLOT FUNCTION**"""

# plot function

plt.style.use('seaborn-whitegrid')

def plot_acc(history):
  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']
  epochs = range(len(acc))
  plt.subplot(1, 2, 1)
  acc_plot, = plt.plot(epochs, acc, 'r')
  val_acc_plot, = plt.plot(epochs, val_acc, 'b')
  plt.title('Training and Validation Accuracy')
  plt.legend([acc_plot, val_acc_plot], ['Training Accuracy', 'Validation Accuracy'])


def plot_loss(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(len(loss))
  plt.subplot(1, 2, 2)
  loss_plot, = plt.plot(epochs, loss, 'r')
  val_loss_plot, = plt.plot(epochs, val_loss, 'b')
  plt.title('Training and Validation Loss')
  plt.legend([loss_plot, val_loss_plot], ['Training Loss', 'Validation Loss'])

def plot_history(history):
  plt.figure(figsize=(15,5))
  plot_acc(history)
  plot_loss(history)

"""**CNN MODELLING**"""

tf.keras.backend.clear_session()

model_simple = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5), 
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])

model_simple.summary()

"""### **RUNNING #1**

**MODEL OPTIMIZER USING ADAM**
"""

model_simple.compile(
    optimizer='adam', 
    loss='categorical_crossentropy', 
    metrics=['accuracy']
)

history_simple = model_simple.fit(
    train_generator,
    epochs=100,
    callbacks = [reduce_lr, early_stop],
    verbose=1,
    validation_data=validation_generator
)

"""**RESULT VISUALIZATION**"""

plot_history(history_simple)

"""**RESULT EVALUATION**"""

validation_generator = validation_datagen.flow_from_directory(
    VALIDATION_DIR,
    batch_size=159,
    class_mode='categorical',
    target_size=(150, 150),
    shuffle = False
)

filenames = validation_generator.filenames
nb_samples = len(filenames)

Y_pred = model_simple.predict_generator(validation_generator, steps = nb_samples)
y_pred = np.argmax(Y_pred, axis=1)

print('Confusion Matrix')
print(confusion_matrix(validation_generator.classes, y_pred))

print('Classification Report')
target_names = ['chicken',
                'spider',
                'sheep',
                'butterfly',
                'elephant',
                'cat']
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))

"""**SAVE TO TF-LITE**"""

model_simple.save_weights('model_simple_weights.h5')
model_simple.save('model_simple.h5')

converter = tf.lite.TFLiteConverter.from_keras_model(model_simple)
tflite_model_simple = converter.convert()

with tf.io.gfile.GFile('model_simple.tflite', 'wb') as f:
  f.write(tflite_model_simple)

"""**MODEL INCEPTION**"""

tf.keras.backend.clear_session()

model_inception = tf.keras.models.Sequential([
    InceptionV3(weights = "imagenet", include_top = False, input_shape = (150, 150, 3)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5), 
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])

model_inception.layers[0].trainable = False

model_inception.summary()

"""### **RUNNING #2**

**MODEL OPTIMIZER USING ADAM**
"""

model_inception.compile(
    optimizer = 'adam', 
    loss = 'categorical_crossentropy', 
    metrics = ['accuracy']
)

history_inception = model_inception.fit(
    train_generator,
    epochs=100,
    callbacks = [reduce_lr, early_stop],
    verbose=1,
    validation_data=validation_generator
)

"""**RESULT VISUALIZATION**"""

plot_history(history_inception)

"""**RESULT EVALUATION**"""

validation_generator = validation_datagen.flow_from_directory(
    VALIDATION_DIR,
    batch_size=159,
    class_mode='categorical',
    target_size=(150, 150),
    shuffle = False
)

filenames = validation_generator.filenames
nb_samples = len(filenames)

Y_pred = model_inception.predict_generator(validation_generator, steps = nb_samples)
y_pred = np.argmax(Y_pred, axis=1)

print('Confusion Matrix')
print(confusion_matrix(validation_generator.classes, y_pred))

print('Classification Report')
target_names = ['chicken',
                'spider',
                'sheep',
                'butterfly',
                'elephant',
                'cat']
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))

"""**SAVE TO TF-LITE**"""

model_inception.save_weights('model_inception_weights.h5')
model_inception.save('model_inception.h5')

converter = tf.lite.TFLiteConverter.from_keras_model(model_inception)
tflite_model_inception = converter.convert()

with tf.io.gfile.GFile('model_inception.tflite', 'wb') as f:
  f.write(tflite_model_inception)

"""### **IMAGE PREDICTION**

**FUNCTION FOR IMAGE PREDICTION**
"""

def predict_image(image_upload, model = model_inception):
  im = image_upload
  im_array = np.asarray(im)
  im_array = im_array*(1/225)
  im_input = tf.reshape(im_array, shape = [1, 150, 150, 3])

  predict_array = model.predict(im_input)[0]

  import pandas as pd
  df = pd.DataFrame(predict_array)
  df = df.rename({0:'Probability'}, axis = 'columns')
  prod = ['butterfly',
          'cat',
          'chicken',
          'elephant',
          'sheep',
          'spider']
  df['Product'] = prod
  df = df[['Product', 'Probability']]

  predict_label = np.argmax(model.predict(im_input))

  if predict_label == 0:
      predict_product = 'Butterfly'
  elif predict_label == 1:
      predict_product = 'Cat'
  elif predict_label == 2:
      predict_product = 'Chicken'
  elif predict_label == 3:
      predict_product = 'Elephant'
  elif predict_label == 4:
      predict_product = 'Sheep'
  else:
      predict_product = 'Spider'

  return predict_product, df

"""**IMAGE PREDICTION**"""

def predict():
  uploaded = files.upload()

  for fn in uploaded.keys():
    path = fn
    img = image.load_img(path, target_size=(150,150))
    imgplot = plt.imshow(img)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    img = np.vstack([x])

  label, df = predict_image(img)

  print('\n')
  plt.show()
  print("\nThe image is detected as " + label)
  print('\n')
  print(df)
  print('\n')

"""**RESULT**"""

predict()