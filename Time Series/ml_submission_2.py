# -*- coding: utf-8 -*-
"""ML Submission-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hCIZu_tjAwmIFVinlJAwAVjvNqUga3tO

# **TIME SERIES FOR ENERGY USAGE DATASET**

---

###**PERSONAL IDENTITY**

Nama : Mukhamad Azis Tholib \
Email: mukhamadazistholib278@gmail.com

###**DOWNLOAD DATASET FROM KAGGLE**
"""

# kaggle instalation package

!pip install -q kaggle

# upload kaggle.json
from google.colab import files
files.upload()

# make directory and change permission for kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# download dataset with 'copy api command' from kaggle

!kaggle datasets download -d mukhamadazistholib/energy-dataset

# unzip dataset
!mkdir timeseries-energydataset
!unzip energy-dataset.zip -d timeseries-energydatase
!ls timeseries-energydatase

"""### **LOAD DATASET**

IMPORT LIBRARY
"""

#import library for analyze

# import pandas
import pandas as pd
import re

import numpy as np
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

#load dataset

data_train = pd.read_csv('timeseries-energydatase/energydata_complete.csv')
data_train

#data train

data_train.isnull().sum()

dates = data_train['date'].values
energy  = data_train['Appliances'].values
     
plt.figure(figsize=(15,5))
plt.plot(dates, energy)
plt.title('Energy Used',
              fontsize=20);

# preproccess - normalize
# scaler = MinMaxScaler(feature_range=(0, 1))
# energy_normalize = scaler.fit_transform(energy.reshape(1,-1))
# energy_norm = energy_normalize[0]
# energy_norm

train_size = int(len(energy) * 0.8)
test_size = len(energy) - train_size
train, test = energy[0:train_size], energy[train_size:len(energy)]
print(len(train),len(test))

minimum = min(train)
maximum = max(train)
print(minimum,maximum)
lower_mae = (maximum - minimum) / 10
print(lower_mae)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(train.astype('float32'), window_size=60, batch_size=100, shuffle_buffer=1000)
train_set

"""### **VALIDATION SET**"""

val_set = windowed_dataset(test.astype('float32'), window_size=60, batch_size=100, shuffle_buffer=1000)
val_set

"""### **LSTM STRUCTURE**"""

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

#callback for mae

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
      if(logs.get('mae') < 45.0):
        print("\nmae is less than 44.5!")
        self.model.stop_training = True
callbacks = myCallback()

"""### **LEARNING RATE OPTIMIZER**"""

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, callbacks=[callbacks], validation_data=val_set, epochs=100)

"""### **PLOTTING**

**MODEL LOSS**
"""

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""**MODEL MAE**"""

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model mae')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()